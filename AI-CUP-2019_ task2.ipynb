{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Pre-trained Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import Library###\n",
    "import pandas as pd\n",
    "import codecs, gc, re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer, get_custom_objects\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.layers import *\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Set file root###\n",
    "CWD = os.getcwd()\n",
    "print(CWD)\n",
    "SEQ_LEN = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'Scibert/bert_config.json'\n",
    "checkpoint_path = 'Scibert/bert_model.ckpt'\n",
    "vocab_path = 'Scibert/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Set up dictionary###\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader :\n",
    "    for line in reader :\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load data###\n",
    "train_df = pd.read_csv(os.path.join(CWD,'data/trainset.csv')).astype(str)\n",
    "val_df = pd.read_csv(os.path.join(CWD,'data/validset.csv')).astype(str)\n",
    "test_df = pd.read_csv(os.path.join(CWD,'data/testset.csv')).astype(str)\n",
    "private_df = pd.read_csv(os.path.join(CWD,'data/task2_private_testset.csv')).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Transform label data to onehot encoding###\n",
    "def label_to_onehot(labels) :\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "\n",
    "###Preprocess data###\n",
    "def process_data(data, test=0) :\n",
    "    all_x1 = []\n",
    "    all_x2 = []\n",
    "    label = []\n",
    "    print(len(data))\n",
    "    for i in range(len(data)) :\n",
    "        sentid = []\n",
    "        senti = []\n",
    "        for sent in data['Abstract'][i].split('$$$') :\n",
    "            x1, x2 = tokenizer.encode(sent)\n",
    "            sentid = sentid + x1\n",
    "            senti = senti + x2\n",
    "        if test == 0:\n",
    "            label.append(label_to_onehot(data['Task 2'][i]))\n",
    "        if len(sentid) > SEQ_LEN :\n",
    "            all_x1.append(sentid[:SEQ_LEN])\n",
    "            all_x2.append(senti[:SEQ_LEN])\n",
    "        else :\n",
    "            all_x1.append(sentid + [0] * (SEQ_LEN - len(sentid)))\n",
    "            all_x2.append(senti + [0] * (SEQ_LEN - len(sentid)))\n",
    "    if test == 0:\n",
    "        return [np.asarray(all_x1), np.asarray(all_x2)], label\n",
    "    else:\n",
    "        return [np.asarray(all_x1), np.asarray(all_x2)]\n",
    "\n",
    "###User define f1-score metric### \n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #if epoch % self.interval == 0:\n",
    "        y_pred = (np.asarray(self.model.predict([self.X_val[0], self.X_val[1]]))).round().astype(int)\n",
    "        #for i in range(len(y_pred)):\n",
    "        #    print(y_pred[i])\n",
    "            #print(self.y_val[i])\n",
    "        score = f1_score(self.y_val, y_pred, average='micro')\n",
    "        print(\"f1_score - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = process_data(train_df)\n",
    "x_val, y_val = process_data(val_df)\n",
    "test_data = process_data(test_df, 1)\n",
    "private_data = process_data(private_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_val = np.asarray(x_val)\n",
    "y_val = np.asarray(y_val)\n",
    "test_data = np.asarray(test_data)\n",
    "private_data = np.asarray(private_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_f1 = IntervalEvaluation(validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        checkpoint_path,\n",
    "        training=True,\n",
    "        trainable=True,\n",
    "        seq_len=SEQ_LEN,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = bert_model.inputs[:2]\n",
    "\n",
    "frontout = bert_model.get_layer('Encoder-4-FeedForward-Norm').output\n",
    "dense1 = GlobalAveragePooling1D()(frontout)\n",
    "\n",
    "modelout = bert_model.get_layer('Encoder-12-FeedForward-Norm').output\n",
    "dense2 = GlobalAveragePooling1D()(modelout)\n",
    "\n",
    "denseout = keras.layers.Maximum()([dense2, dense1])\n",
    "\n",
    "denseout = Dropout(0.1)(denseout)\n",
    "\n",
    "outputs = Dense(4, activation='sigmoid')(denseout)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-5), metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='save/weights.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1e-5\n",
    "history = model.fit([x_train[0], x_train[1]], y_train, batch_size=4, epochs=1, validation_data=([x_val[0], x_val[1]],y_val), callbacks=[get_f1,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1e-6\n",
    "history = model.fit([x_train[0], x_train[1]], y_train, batch_size=4, epochs=1, validation_data=([x_val[0], x_val[1]],y_val), callbacks=[get_f1,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###1e-7\n",
    "history = model.fit([x_train[0], x_train[1]], y_train, batch_size=4, epochs=1, validation_data=([x_val[0], x_val[1]],y_val), callbacks=[get_f1,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1e-7\n",
    "history = model.fit([x_train[0], x_train[1]], y_train, batch_size=4, epochs=1, validation_data=([x_val[0], x_val[1]],y_val), callbacks=[get_f1,checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('save/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('save/weights_best.h5', custom_objects=get_custom_objects())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Public dataset testing###\n",
    "ans_pred = model.predict([test_data[0], test_data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Private dataset testing###\n",
    "privans_pred = model.predict([private_data[0], private_data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Validation dataset testing###\n",
    "valpred = model.predict([x_val[0], x_val[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Function to modify the threshold of sigmoid output###\n",
    "def mod_round(data, threshold=0.5):\n",
    "    pred = []\n",
    "    for i in range(len(data)) :\n",
    "        pred_line = []\n",
    "        for j in range(4) :\n",
    "            if(data[i][j]) > threshold :\n",
    "                pred_line += [1]\n",
    "            else :\n",
    "                pred_line += [0]\n",
    "        pred.append(pred_line)\n",
    "    pred = np.asarray(pred).astype(int)\n",
    "    return pred\n",
    "\n",
    "###Fix the unreasonable prediction###\n",
    "def fix_ans(data):\n",
    "    wrong1 = 0\n",
    "    wrong2 = 0\n",
    "    others = 0\n",
    "    index2 = 0\n",
    "    for i in range(len(data)):\n",
    "        if sum(data[i]) == 0:\n",
    "            wrong1 += 1\n",
    "            data[i] = [0,0,0,1]\n",
    "        if sum(data[i][0:3]) > 0 and data[i][3] == 1:\n",
    "            wrong2 += 1\n",
    "            data[i][3] = 0\n",
    "        if data[i][3] == 1 :\n",
    "            others += 1\n",
    "    return data, wrong1, wrong2, others\n",
    "\n",
    "###Calculate the F1-score when testing with validation data###\n",
    "def cal_score(data, y_val) :\n",
    "    TP = 0\n",
    "    AP = 0\n",
    "    TL = 0\n",
    "    for i in range(len(data)) :\n",
    "        TP = TP + (data[i]*y_val[i]).sum()\n",
    "        AP = AP + (data[i].sum())\n",
    "        TL = TL + (y_val[i]).sum()\n",
    "    precision = TP / AP\n",
    "    recall = TP / TL\n",
    "    FS = 2*precision*recall/(precision+recall)\n",
    "    #print(TP, AP, TL)\n",
    "    return precision, recall, FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Find the outlier of testing data\n",
    "outlier = []\n",
    "for i in range(len(private_df)) :\n",
    "    if len(private_df['Abstract'][i]) < 500:\n",
    "        if len(private_df['Abstract'][i]) < 30:\n",
    "            outlier.append(i)\n",
    "            print(i+1)\n",
    "        else :\n",
    "            for sents in private_df['Abstract'][i].split('$$$') :\n",
    "                for sent in re.split('.\\s',sents) :\n",
    "                    if sent == 'withdrawn' :\n",
    "                        print(i+1)\n",
    "                        outlier.append(i)\n",
    "                    elif sent[0:-1] == 'withdrawn' :\n",
    "                        print(i+1)\n",
    "                        outlier.append(i)\n",
    "                    elif sent == 'withdraw' :\n",
    "                        print(i+1)\n",
    "                        outlier.append(i)\n",
    "                    elif sent[0:-1] == 'withdraw' :\n",
    "                        print(i+1)\n",
    "                        outlier.append(i)\n",
    "print(outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mod_round(privans_pred,0.419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, wrong1, wrong2, others = fix_ans(pred)\n",
    "print(pred, wrong1, wrong2, others, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Fix the outlier of test data###\n",
    "for index in outlier : \n",
    "    pred[index] = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###To calculate f1-score of validation dataset###\n",
    "print(cal_score(pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###List the f1-score with different threshold on validation data###\n",
    "table = []\n",
    "for i in range(400,500,1) :\n",
    "    pred = mod_round(val1pred, i/1000)\n",
    "    pred, wrong1, wrong2, others, index2 = fix_ans(pred)\n",
    "    recision, recall, FS = cal_score(pred, y_val)\n",
    "    table.append([i/1000, recision, recall, FS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submit File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubmitGenerator(pred, \n",
    "                os.path.join(CWD,'data/task2_sample_submission.csv'),\n",
    "                False, \n",
    "                os.path.join(CWD,'task2_submission_0102.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
